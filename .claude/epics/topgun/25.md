---
name: Implement SystemResourceMonitor service with metric aggregation
status: open
created: 2025-10-06T15:23:47Z
updated: 2025-10-06T20:38:44Z
github: https://github.com/johnproblems/topgun/issues/135
depends_on: [23]
parallel: false
conflicts_with: []
---

# Task: Implement SystemResourceMonitor service with metric aggregation

## Description

Build a comprehensive **SystemResourceMonitor** service that provides time-series metric aggregation, historical data storage, intelligent query optimization, and real-time metric analysis for the Coolify Enterprise resource monitoring system. This service acts as the central data aggregation layer between raw metric collection (from Task 23's enhanced ResourcesCheck) and dashboard visualization (Task 29's ResourceDashboard.vue).

The SystemResourceMonitor service is a critical component that transforms raw real-time server metrics into actionable insights for capacity planning, performance optimization, and resource allocation decisions. It provides the analytical foundation for the entire enterprise resource management system.

**Key Responsibilities:**

1. **Time-Series Data Management**
   - Aggregate raw metrics from `server_resource_metrics` table into time-bucketed summaries
   - Store aggregated data with multiple time granularities (1-minute, 5-minute, 1-hour, 1-day)
   - Implement intelligent data retention policies to manage database growth
   - Provide efficient queries for historical trend analysis

2. **Real-Time Metric Analysis**
   - Calculate rolling averages, percentiles, and statistical summaries
   - Detect anomalies and performance degradation patterns
   - Track resource utilization trends over time
   - Generate capacity forecast predictions

3. **Organization-Level Aggregation**
   - Roll up server metrics to organization level for quota enforcement
   - Support hierarchical organization aggregation (sub-orgs â†’ parent org)
   - Calculate organization-wide resource utilization
   - Track resource usage against license quotas

4. **Performance Optimization**
   - Implement Redis caching for frequently accessed metrics
   - Use database partitioning for time-series tables
   - Optimize queries with proper indexes and aggregation pipelines
   - Support efficient pagination for large metric datasets

**Integration with Coolify Enterprise Architecture:**

- **Consumes:** Raw metrics from Task 23 (ResourcesCheck enhancement) stored in `server_resource_metrics` table
- **Provides:** Aggregated metrics to Task 29 (ResourceDashboard.vue) and Task 30 (CapacityPlanner.vue)
- **Supports:** Task 26 (CapacityManager) for intelligent server selection based on historical trends
- **Enables:** Task 28 (Organization quota enforcement) with real-time usage tracking

**Why this task is critical:** Without intelligent metric aggregation, the system would accumulate millions of raw data points making queries slow and dashboards unresponsive. The SystemResourceMonitor provides the data analytics layer that makes real-time resource monitoring scalable for thousands of servers across hundreds of organizations. This service is essential for capacity planning, performance optimization, and enabling data-driven infrastructure decisions.

**Performance Requirements:**
- Query response time for dashboard data: < 200ms (95th percentile)
- Support for 10,000+ servers with 30-second metric collection intervals
- Aggregation job execution time: < 5 minutes for 1-hour aggregations
- Redis cache hit rate: > 80% for recent metrics
- Database storage efficiency: 60% reduction through aggregation

## Acceptance Criteria

- [ ] SystemResourceMonitor service implements time-series metric aggregation with multiple granularities
- [ ] Support for 1-minute, 5-minute, 1-hour, 1-day aggregation intervals
- [ ] Aggregate metrics include: avg, min, max, p50, p95, p99 for CPU, memory, disk, network
- [ ] Historical query methods with date range filtering and pagination support
- [ ] Organization-level metric aggregation with hierarchical rollup
- [ ] Redis caching layer for recent metrics (last 15 minutes cached)
- [ ] Data retention policies: raw data (7 days), 1-min agg (30 days), 1-hour agg (1 year), 1-day agg (forever)
- [ ] Anomaly detection for sudden metric spikes or drops (> 50% change)
- [ ] Capacity forecasting based on historical trends (linear regression)
- [ ] Database query optimization with proper indexes on time-series tables
- [ ] Background job for periodic aggregation (AggregateMetricsJob)
- [ ] API endpoints for metric retrieval with filtering and pagination
- [ ] Performance benchmark: < 200ms for dashboard queries (1000 data points)
- [ ] Support for server comparison queries (compare multiple servers side-by-side)
- [ ] Export functionality for metrics data (CSV, JSON formats)

## Technical Details

### File Paths

**Service Layer:**
- `/home/topgun/topgun/app/Services/Enterprise/SystemResourceMonitor.php` (new)
- `/home/topgun/topgun/app/Contracts/SystemResourceMonitorInterface.php` (new)

**Background Jobs:**
- `/home/topgun/topgun/app/Jobs/Enterprise/AggregateMetricsJob.php` (new)
- `/home/topgun/topgun/app/Jobs/Enterprise/CleanupOldMetricsJob.php` (new)

**Models:**
- `/home/topgun/topgun/app/Models/Enterprise/ServerResourceMetric.php` (modify - add aggregation scopes)
- `/home/topgun/topgun/app/Models/Enterprise/MetricAggregation.php` (new - for aggregated data)
- `/home/topgun/topgun/app/Models/Enterprise/OrganizationResourceUsage.php` (modify - add aggregation methods)

**Controllers:**
- `/home/topgun/topgun/app/Http/Controllers/Api/Enterprise/MetricsController.php` (new)

**Database:**
- `/home/topgun/topgun/database/migrations/2025_10_06_create_metric_aggregations_table.php` (new)

### Database Schema Enhancement

Create new table for aggregated metrics:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up(): void
    {
        Schema::create('metric_aggregations', function (Blueprint $table) {
            $table->id();
            $table->foreignId('server_id')->constrained()->cascadeOnDelete();
            $table->foreignId('organization_id')->constrained()->cascadeOnDelete();

            // Time bucketing
            $table->timestamp('bucket_start');
            $table->timestamp('bucket_end');
            $table->enum('granularity', ['1min', '5min', '1hour', '1day'])->default('1min');

            // CPU metrics (percentages)
            $table->decimal('cpu_avg', 5, 2)->nullable();
            $table->decimal('cpu_min', 5, 2)->nullable();
            $table->decimal('cpu_max', 5, 2)->nullable();
            $table->decimal('cpu_p50', 5, 2)->nullable();
            $table->decimal('cpu_p95', 5, 2)->nullable();
            $table->decimal('cpu_p99', 5, 2)->nullable();

            // Memory metrics (bytes)
            $table->bigInteger('memory_avg')->nullable();
            $table->bigInteger('memory_min')->nullable();
            $table->bigInteger('memory_max')->nullable();
            $table->bigInteger('memory_p50')->nullable();
            $table->bigInteger('memory_p95')->nullable();
            $table->bigInteger('memory_p99')->nullable();

            // Disk metrics (bytes)
            $table->bigInteger('disk_avg')->nullable();
            $table->bigInteger('disk_min')->nullable();
            $table->bigInteger('disk_max')->nullable();
            $table->bigInteger('disk_p50')->nullable();
            $table->bigInteger('disk_p95')->nullable();
            $table->bigInteger('disk_p99')->nullable();

            // Network metrics (bytes/second)
            $table->bigInteger('network_in_avg')->nullable();
            $table->bigInteger('network_in_max')->nullable();
            $table->bigInteger('network_out_avg')->nullable();
            $table->bigInteger('network_out_max')->nullable();

            // Metadata
            $table->integer('sample_count')->default(0); // Number of raw samples aggregated

            $table->timestamps();

            // Indexes for efficient time-series queries
            $table->index(['server_id', 'granularity', 'bucket_start']);
            $table->index(['organization_id', 'granularity', 'bucket_start']);
            $table->index(['bucket_start', 'bucket_end']);
            $table->index('granularity');

            // Unique constraint to prevent duplicate aggregations
            $table->unique(['server_id', 'granularity', 'bucket_start'], 'unique_server_granularity_bucket');
        });

        // Add partition support for PostgreSQL (optional but recommended)
        if (DB::getDriverName() === 'pgsql') {
            DB::statement("
                -- Convert to partitioned table by granularity
                -- This significantly improves query performance for time-series data
                -- Partitions will be created for each granularity level
            ");
        }
    }

    public function down(): void
    {
        Schema::dropIfExists('metric_aggregations');
    }
};
```

### SystemResourceMonitor Service Implementation

**File:** `app/Services/Enterprise/SystemResourceMonitor.php`

```php
<?php

namespace App\Services\Enterprise;

use App\Contracts\SystemResourceMonitorInterface;
use App\Models\Server;
use App\Models\Organization;
use App\Models\Enterprise\ServerResourceMetric;
use App\Models\Enterprise\MetricAggregation;
use App\Models\Enterprise\OrganizationResourceUsage;
use Illuminate\Support\Collection;
use Illuminate\Support\Facades\Cache;
use Illuminate\Support\Facades\DB;
use Illuminate\Support\Facades\Log;
use Carbon\Carbon;

class SystemResourceMonitor implements SystemResourceMonitorInterface
{
    /**
     * Cache configuration
     */
    private const CACHE_PREFIX = 'metrics';
    private const CACHE_TTL_RECENT = 300; // 5 minutes for recent metrics
    private const CACHE_TTL_HISTORICAL = 3600; // 1 hour for historical data

    /**
     * Data retention policies (in days)
     */
    private const RETENTION_RAW = 7;
    private const RETENTION_1MIN = 30;
    private const RETENTION_1HOUR = 365;
    private const RETENTION_1DAY = null; // Keep forever

    /**
     * Anomaly detection threshold
     */
    private const ANOMALY_THRESHOLD = 0.50; // 50% change

    /**
     * Get server metrics for a specific time range with aggregation
     *
     * @param Server $server
     * @param Carbon $startTime
     * @param Carbon $endTime
     * @param string $granularity
     * @return Collection
     */
    public function getServerMetrics(
        Server $server,
        Carbon $startTime,
        Carbon $endTime,
        string $granularity = '5min'
    ): Collection {
        $cacheKey = $this->getCacheKey('server', $server->id, $startTime, $endTime, $granularity);

        return Cache::remember($cacheKey, self::CACHE_TTL_RECENT, function () use ($server, $startTime, $endTime, $granularity) {
            return MetricAggregation::where('server_id', $server->id)
                ->where('granularity', $granularity)
                ->whereBetween('bucket_start', [$startTime, $endTime])
                ->orderBy('bucket_start')
                ->get()
                ->map(function ($metric) {
                    return [
                        'timestamp' => $metric->bucket_start->toIso8601String(),
                        'cpu' => [
                            'avg' => $metric->cpu_avg,
                            'min' => $metric->cpu_min,
                            'max' => $metric->cpu_max,
                            'p95' => $metric->cpu_p95,
                        ],
                        'memory' => [
                            'avg' => $metric->memory_avg,
                            'min' => $metric->memory_min,
                            'max' => $metric->memory_max,
                            'p95' => $metric->memory_p95,
                        ],
                        'disk' => [
                            'avg' => $metric->disk_avg,
                            'min' => $metric->disk_min,
                            'max' => $metric->disk_max,
                            'p95' => $metric->disk_p95,
                        ],
                        'network' => [
                            'in_avg' => $metric->network_in_avg,
                            'in_max' => $metric->network_in_max,
                            'out_avg' => $metric->network_out_avg,
                            'out_max' => $metric->network_out_max,
                        ],
                    ];
                });
        });
    }

    /**
     * Get organization-wide aggregated metrics
     *
     * @param Organization $organization
     * @param Carbon $startTime
     * @param Carbon $endTime
     * @param string $granularity
     * @return Collection
     */
    public function getOrganizationMetrics(
        Organization $organization,
        Carbon $startTime,
        Carbon $endTime,
        string $granularity = '1hour'
    ): Collection {
        $cacheKey = $this->getCacheKey('org', $organization->id, $startTime, $endTime, $granularity);

        return Cache::remember($cacheKey, self::CACHE_TTL_HISTORICAL, function () use ($organization, $startTime, $endTime, $granularity) {
            // Get all servers for the organization
            $serverIds = $organization->servers()->pluck('id');

            // Aggregate metrics across all servers
            return MetricAggregation::whereIn('server_id', $serverIds)
                ->where('granularity', $granularity)
                ->whereBetween('bucket_start', [$startTime, $endTime])
                ->select([
                    'bucket_start',
                    DB::raw('AVG(cpu_avg) as cpu_avg'),
                    DB::raw('MAX(cpu_max) as cpu_max'),
                    DB::raw('SUM(memory_avg) as memory_total'),
                    DB::raw('SUM(disk_avg) as disk_total'),
                    DB::raw('SUM(network_in_avg) as network_in_total'),
                    DB::raw('SUM(network_out_avg) as network_out_total'),
                ])
                ->groupBy('bucket_start')
                ->orderBy('bucket_start')
                ->get()
                ->map(function ($metric) {
                    return [
                        'timestamp' => $metric->bucket_start->toIso8601String(),
                        'cpu_avg' => round($metric->cpu_avg, 2),
                        'cpu_max' => round($metric->cpu_max, 2),
                        'memory_total' => $metric->memory_total,
                        'disk_total' => $metric->disk_total,
                        'network_in_total' => $metric->network_in_total,
                        'network_out_total' => $metric->network_out_total,
                    ];
                });
        });
    }

    /**
     * Aggregate raw metrics into time buckets
     *
     * @param string $granularity
     * @param Carbon $startTime
     * @param Carbon $endTime
     * @return int Number of aggregations created
     */
    public function aggregateMetrics(
        string $granularity,
        Carbon $startTime,
        Carbon $endTime
    ): int {
        $bucketSize = $this->getBucketSizeInMinutes($granularity);
        $aggregationsCreated = 0;

        $currentBucket = $startTime->copy()->floorMinutes($bucketSize);

        while ($currentBucket->lt($endTime)) {
            $bucketEnd = $currentBucket->copy()->addMinutes($bucketSize);

            // Get all servers with metrics in this time bucket
            $serverMetrics = ServerResourceMetric::whereBetween('collected_at', [$currentBucket, $bucketEnd])
                ->select([
                    'server_id',
                    DB::raw('AVG(cpu_usage_percent) as cpu_avg'),
                    DB::raw('MIN(cpu_usage_percent) as cpu_min'),
                    DB::raw('MAX(cpu_usage_percent) as cpu_max'),
                    DB::raw($this->getPercentileQuery('cpu_usage_percent', 50) . ' as cpu_p50'),
                    DB::raw($this->getPercentileQuery('cpu_usage_percent', 95) . ' as cpu_p95'),
                    DB::raw($this->getPercentileQuery('cpu_usage_percent', 99) . ' as cpu_p99'),

                    DB::raw('AVG(memory_used_bytes) as memory_avg'),
                    DB::raw('MIN(memory_used_bytes) as memory_min'),
                    DB::raw('MAX(memory_used_bytes) as memory_max'),
                    DB::raw($this->getPercentileQuery('memory_used_bytes', 50) . ' as memory_p50'),
                    DB::raw($this->getPercentileQuery('memory_used_bytes', 95) . ' as memory_p95'),
                    DB::raw($this->getPercentileQuery('memory_used_bytes', 99) . ' as memory_p99'),

                    DB::raw('AVG(disk_used_bytes) as disk_avg'),
                    DB::raw('MIN(disk_used_bytes) as disk_min'),
                    DB::raw('MAX(disk_used_bytes) as disk_max'),
                    DB::raw($this->getPercentileQuery('disk_used_bytes', 50) . ' as disk_p50'),
                    DB::raw($this->getPercentileQuery('disk_used_bytes', 95) . ' as disk_p95'),
                    DB::raw($this->getPercentileQuery('disk_used_bytes', 99) . ' as disk_p99'),

                    DB::raw('AVG(network_in_bytes_per_sec) as network_in_avg'),
                    DB::raw('MAX(network_in_bytes_per_sec) as network_in_max'),
                    DB::raw('AVG(network_out_bytes_per_sec) as network_out_avg'),
                    DB::raw('MAX(network_out_bytes_per_sec) as network_out_max'),

                    DB::raw('COUNT(*) as sample_count'),
                ])
                ->groupBy('server_id')
                ->get();

            // Create aggregation records
            foreach ($serverMetrics as $serverMetric) {
                $server = Server::find($serverMetric->server_id);

                if (!$server) {
                    continue;
                }

                MetricAggregation::updateOrCreate(
                    [
                        'server_id' => $server->id,
                        'granularity' => $granularity,
                        'bucket_start' => $currentBucket,
                    ],
                    [
                        'organization_id' => $server->team->id, // Using team as organization
                        'bucket_end' => $bucketEnd,
                        'cpu_avg' => $serverMetric->cpu_avg,
                        'cpu_min' => $serverMetric->cpu_min,
                        'cpu_max' => $serverMetric->cpu_max,
                        'cpu_p50' => $serverMetric->cpu_p50,
                        'cpu_p95' => $serverMetric->cpu_p95,
                        'cpu_p99' => $serverMetric->cpu_p99,
                        'memory_avg' => $serverMetric->memory_avg,
                        'memory_min' => $serverMetric->memory_min,
                        'memory_max' => $serverMetric->memory_max,
                        'memory_p50' => $serverMetric->memory_p50,
                        'memory_p95' => $serverMetric->memory_p95,
                        'memory_p99' => $serverMetric->memory_p99,
                        'disk_avg' => $serverMetric->disk_avg,
                        'disk_min' => $serverMetric->disk_min,
                        'disk_max' => $serverMetric->disk_max,
                        'disk_p50' => $serverMetric->disk_p50,
                        'disk_p95' => $serverMetric->disk_p95,
                        'disk_p99' => $serverMetric->disk_p99,
                        'network_in_avg' => $serverMetric->network_in_avg,
                        'network_in_max' => $serverMetric->network_in_max,
                        'network_out_avg' => $serverMetric->network_out_avg,
                        'network_out_max' => $serverMetric->network_out_max,
                        'sample_count' => $serverMetric->sample_count,
                    ]
                );

                $aggregationsCreated++;
            }

            $currentBucket = $bucketEnd;
        }

        Log::info("Aggregated {$aggregationsCreated} metric buckets for {$granularity} granularity");

        return $aggregationsCreated;
    }

    /**
     * Detect anomalies in server metrics
     *
     * @param Server $server
     * @param Carbon $checkTime
     * @return array
     */
    public function detectAnomalies(Server $server, Carbon $checkTime): array
    {
        $anomalies = [];

        // Get baseline metrics (last hour average)
        $baselineStart = $checkTime->copy()->subHour();
        $baselineMetrics = $this->getServerMetrics($server, $baselineStart, $checkTime, '5min');

        if ($baselineMetrics->isEmpty()) {
            return $anomalies;
        }

        $baselineAvg = [
            'cpu' => $baselineMetrics->avg('cpu.avg'),
            'memory' => $baselineMetrics->avg('memory.avg'),
            'disk' => $baselineMetrics->avg('disk.avg'),
        ];

        // Get most recent metric
        $recentMetric = ServerResourceMetric::where('server_id', $server->id)
            ->where('collected_at', '>=', $checkTime->copy()->subMinutes(5))
            ->orderBy('collected_at', 'desc')
            ->first();

        if (!$recentMetric) {
            return $anomalies;
        }

        // Check for significant deviations
        if ($this->isAnomaly($recentMetric->cpu_usage_percent, $baselineAvg['cpu'])) {
            $anomalies[] = [
                'metric' => 'cpu',
                'current' => $recentMetric->cpu_usage_percent,
                'baseline' => $baselineAvg['cpu'],
                'change_percent' => $this->calculateChangePercent($recentMetric->cpu_usage_percent, $baselineAvg['cpu']),
            ];
        }

        if ($this->isAnomaly($recentMetric->memory_used_bytes, $baselineAvg['memory'])) {
            $anomalies[] = [
                'metric' => 'memory',
                'current' => $recentMetric->memory_used_bytes,
                'baseline' => $baselineAvg['memory'],
                'change_percent' => $this->calculateChangePercent($recentMetric->memory_used_bytes, $baselineAvg['memory']),
            ];
        }

        if ($this->isAnomaly($recentMetric->disk_used_bytes, $baselineAvg['disk'])) {
            $anomalies[] = [
                'metric' => 'disk',
                'current' => $recentMetric->disk_used_bytes,
                'baseline' => $baselineAvg['disk'],
                'change_percent' => $this->calculateChangePercent($recentMetric->disk_used_bytes, $baselineAvg['disk']),
            ];
        }

        return $anomalies;
    }

    /**
     * Generate capacity forecast for server
     *
     * @param Server $server
     * @param string $metric
     * @param int $forecastDays
     * @return array
     */
    public function generateCapacityForecast(
        Server $server,
        string $metric = 'cpu',
        int $forecastDays = 7
    ): array {
        // Get historical data for the last 30 days
        $endTime = Carbon::now();
        $startTime = $endTime->copy()->subDays(30);

        $historicalData = $this->getServerMetrics($server, $startTime, $endTime, '1day');

        if ($historicalData->count() < 7) {
            return [
                'forecast' => [],
                'trend' => 'insufficient_data',
                'confidence' => 0,
            ];
        }

        // Simple linear regression for forecasting
        $dataPoints = $historicalData->map(function ($item) use ($metric) {
            return [
                'x' => Carbon::parse($item['timestamp'])->timestamp,
                'y' => $item[$metric]['avg'],
            ];
        })->values();

        $regression = $this->linearRegression($dataPoints);

        // Generate forecast
        $forecast = [];
        $lastTimestamp = Carbon::parse($historicalData->last()['timestamp']);

        for ($i = 1; $i <= $forecastDays; $i++) {
            $forecastTime = $lastTimestamp->copy()->addDays($i);
            $forecastValue = $regression['slope'] * $forecastTime->timestamp + $regression['intercept'];

            $forecast[] = [
                'timestamp' => $forecastTime->toIso8601String(),
                'value' => max(0, round($forecastValue, 2)), // Ensure non-negative
            ];
        }

        return [
            'forecast' => $forecast,
            'trend' => $regression['slope'] > 0 ? 'increasing' : 'decreasing',
            'confidence' => $regression['r_squared'],
        ];
    }

    /**
     * Clean up old metrics based on retention policies
     *
     * @return array Cleanup statistics
     */
    public function cleanupOldMetrics(): array
    {
        $stats = [
            'raw_deleted' => 0,
            '1min_deleted' => 0,
            '1hour_deleted' => 0,
        ];

        // Delete raw metrics older than 7 days
        if (self::RETENTION_RAW !== null) {
            $rawCutoff = Carbon::now()->subDays(self::RETENTION_RAW);
            $stats['raw_deleted'] = ServerResourceMetric::where('collected_at', '<', $rawCutoff)->delete();
        }

        // Delete 1-minute aggregations older than 30 days
        if (self::RETENTION_1MIN !== null) {
            $minCutoff = Carbon::now()->subDays(self::RETENTION_1MIN);
            $stats['1min_deleted'] = MetricAggregation::where('granularity', '1min')
                ->where('bucket_start', '<', $minCutoff)
                ->delete();
        }

        // Delete 1-hour aggregations older than 365 days
        if (self::RETENTION_1HOUR !== null) {
            $hourCutoff = Carbon::now()->subDays(self::RETENTION_1HOUR);
            $stats['1hour_deleted'] = MetricAggregation::where('granularity', '1hour')
                ->where('bucket_start', '<', $hourCutoff)
                ->delete();
        }

        Log::info("Cleaned up old metrics", $stats);

        return $stats;
    }

    /**
     * Update organization resource usage totals
     *
     * @param Organization $organization
     * @return void
     */
    public function updateOrganizationUsage(Organization $organization): void
    {
        $servers = $organization->servers;

        // Get most recent metrics for each server
        $totalCpu = 0;
        $totalMemory = 0;
        $totalDisk = 0;
        $serverCount = 0;

        foreach ($servers as $server) {
            $recentMetric = ServerResourceMetric::where('server_id', $server->id)
                ->orderBy('collected_at', 'desc')
                ->first();

            if ($recentMetric) {
                $totalCpu += $recentMetric->cpu_usage_percent;
                $totalMemory += $recentMetric->memory_used_bytes;
                $totalDisk += $recentMetric->disk_used_bytes;
                $serverCount++;
            }
        }

        // Update or create organization usage record
        OrganizationResourceUsage::updateOrCreate(
            [
                'organization_id' => $organization->id,
                'period_start' => Carbon::now()->startOfHour(),
            ],
            [
                'server_count' => $serverCount,
                'cpu_average' => $serverCount > 0 ? $totalCpu / $serverCount : 0,
                'memory_total_bytes' => $totalMemory,
                'disk_total_bytes' => $totalDisk,
            ]
        );
    }

    /**
     * Get cache key for metrics
     *
     * @param string $type
     * @param int $id
     * @param Carbon $start
     * @param Carbon $end
     * @param string $granularity
     * @return string
     */
    private function getCacheKey(string $type, int $id, Carbon $start, Carbon $end, string $granularity): string
    {
        return sprintf(
            '%s:%s:%d:%s:%s:%s',
            self::CACHE_PREFIX,
            $type,
            $id,
            $start->timestamp,
            $end->timestamp,
            $granularity
        );
    }

    /**
     * Get bucket size in minutes for granularity
     *
     * @param string $granularity
     * @return int
     */
    private function getBucketSizeInMinutes(string $granularity): int
    {
        return match ($granularity) {
            '1min' => 1,
            '5min' => 5,
            '1hour' => 60,
            '1day' => 1440,
            default => 5,
        };
    }

    /**
     * Get database-specific percentile query
     *
     * @param string $column
     * @param int $percentile
     * @return string
     */
    private function getPercentileQuery(string $column, int $percentile): string
    {
        $driver = DB::getDriverName();

        return match ($driver) {
            'pgsql' => "PERCENTILE_CONT({$percentile}/100.0) WITHIN GROUP (ORDER BY {$column})",
            'mysql' => "SUBSTRING_INDEX(SUBSTRING_INDEX(GROUP_CONCAT({$column} ORDER BY {$column} SEPARATOR ','), ',', {$percentile}/100.0 * COUNT(*)), ',', -1)",
            default => "AVG({$column})", // Fallback to average
        };
    }

    /**
     * Check if value is an anomaly compared to baseline
     *
     * @param float $current
     * @param float $baseline
     * @return bool
     */
    private function isAnomaly(float $current, float $baseline): bool
    {
        if ($baseline == 0) {
            return $current > 0;
        }

        $changePercent = abs(($current - $baseline) / $baseline);

        return $changePercent > self::ANOMALY_THRESHOLD;
    }

    /**
     * Calculate percentage change
     *
     * @param float $current
     * @param float $baseline
     * @return float
     */
    private function calculateChangePercent(float $current, float $baseline): float
    {
        if ($baseline == 0) {
            return 100;
        }

        return round((($current - $baseline) / $baseline) * 100, 2);
    }

    /**
     * Simple linear regression
     *
     * @param Collection $dataPoints
     * @return array
     */
    private function linearRegression(Collection $dataPoints): array
    {
        $n = $dataPoints->count();

        $sumX = $dataPoints->sum('x');
        $sumY = $dataPoints->sum('y');
        $sumXY = $dataPoints->reduce(fn($carry, $point) => $carry + ($point['x'] * $point['y']), 0);
        $sumX2 = $dataPoints->reduce(fn($carry, $point) => $carry + ($point['x'] ** 2), 0);
        $sumY2 = $dataPoints->reduce(fn($carry, $point) => $carry + ($point['y'] ** 2), 0);

        $slope = ($n * $sumXY - $sumX * $sumY) / ($n * $sumX2 - $sumX ** 2);
        $intercept = ($sumY - $slope * $sumX) / $n;

        // Calculate R-squared
        $yMean = $sumY / $n;
        $ssTotal = $dataPoints->reduce(fn($carry, $point) => $carry + (($point['y'] - $yMean) ** 2), 0);
        $ssResidual = $dataPoints->reduce(fn($carry, $point) => $carry + (($point['y'] - ($slope * $point['x'] + $intercept)) ** 2), 0);
        $rSquared = $ssTotal > 0 ? 1 - ($ssResidual / $ssTotal) : 0;

        return [
            'slope' => $slope,
            'intercept' => $intercept,
            'r_squared' => round($rSquared, 4),
        ];
    }
}
```

### Service Interface

**File:** `app/Contracts/SystemResourceMonitorInterface.php`

```php
<?php

namespace App\Contracts;

use App\Models\Server;
use App\Models\Organization;
use Illuminate\Support\Collection;
use Carbon\Carbon;

interface SystemResourceMonitorInterface
{
    /**
     * Get server metrics for a time range
     *
     * @param Server $server
     * @param Carbon $startTime
     * @param Carbon $endTime
     * @param string $granularity
     * @return Collection
     */
    public function getServerMetrics(
        Server $server,
        Carbon $startTime,
        Carbon $endTime,
        string $granularity = '5min'
    ): Collection;

    /**
     * Get organization-wide metrics
     *
     * @param Organization $organization
     * @param Carbon $startTime
     * @param Carbon $endTime
     * @param string $granularity
     * @return Collection
     */
    public function getOrganizationMetrics(
        Organization $organization,
        Carbon $startTime,
        Carbon $endTime,
        string $granularity = '1hour'
    ): Collection;

    /**
     * Aggregate raw metrics into time buckets
     *
     * @param string $granularity
     * @param Carbon $startTime
     * @param Carbon $endTime
     * @return int
     */
    public function aggregateMetrics(
        string $granularity,
        Carbon $startTime,
        Carbon $endTime
    ): int;

    /**
     * Detect anomalies in server metrics
     *
     * @param Server $server
     * @param Carbon $checkTime
     * @return array
     */
    public function detectAnomalies(Server $server, Carbon $checkTime): array;

    /**
     * Generate capacity forecast
     *
     * @param Server $server
     * @param string $metric
     * @param int $forecastDays
     * @return array
     */
    public function generateCapacityForecast(
        Server $server,
        string $metric = 'cpu',
        int $forecastDays = 7
    ): array;

    /**
     * Clean up old metrics
     *
     * @return array
     */
    public function cleanupOldMetrics(): array;

    /**
     * Update organization resource usage
     *
     * @param Organization $organization
     * @return void
     */
    public function updateOrganizationUsage(Organization $organization): void;
}
```

### Background Job - AggregateMetricsJob

**File:** `app/Jobs/Enterprise/AggregateMetricsJob.php`

```php
<?php

namespace App\Jobs\Enterprise;

use App\Contracts\SystemResourceMonitorInterface;
use Carbon\Carbon;
use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Illuminate\Support\Facades\Log;

class AggregateMetricsJob implements ShouldQueue
{
    use Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    public function __construct(
        private string $granularity,
        private ?Carbon $startTime = null,
        private ?Carbon $endTime = null
    ) {
        $this->onQueue('monitoring');
    }

    public function handle(SystemResourceMonitorInterface $monitor): void
    {
        $endTime = $this->endTime ?? Carbon::now();

        // Default start time based on granularity
        $startTime = $this->startTime ?? match ($this->granularity) {
            '1min' => $endTime->copy()->subMinutes(5),
            '5min' => $endTime->copy()->subHour(),
            '1hour' => $endTime->copy()->subDay(),
            '1day' => $endTime->copy()->subWeek(),
            default => $endTime->copy()->subHour(),
        };

        Log::info("Starting metric aggregation: {$this->granularity}", [
            'start' => $startTime->toDateTimeString(),
            'end' => $endTime->toDateTimeString(),
        ]);

        $aggregationsCreated = $monitor->aggregateMetrics(
            $this->granularity,
            $startTime,
            $endTime
        );

        Log::info("Completed metric aggregation: {$this->granularity}", [
            'aggregations_created' => $aggregationsCreated,
        ]);
    }

    public function tags(): array
    {
        return ['metrics', 'aggregation', $this->granularity];
    }
}
```

### Background Job - CleanupOldMetricsJob

**File:** `app/Jobs/Enterprise/CleanupOldMetricsJob.php`

```php
<?php

namespace App\Jobs\Enterprise;

use App\Contracts\SystemResourceMonitorInterface;
use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Illuminate\Support\Facades\Log;

class CleanupOldMetricsJob implements ShouldQueue
{
    use Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    public function __construct()
    {
        $this->onQueue('monitoring');
    }

    public function handle(SystemResourceMonitorInterface $monitor): void
    {
        Log::info("Starting old metrics cleanup");

        $stats = $monitor->cleanupOldMetrics();

        Log::info("Completed old metrics cleanup", $stats);
    }

    public function tags(): array
    {
        return ['metrics', 'cleanup'];
    }
}
```

### API Controller

**File:** `app/Http/Controllers/Api/Enterprise/MetricsController.php`

```php
<?php

namespace App\Http\Controllers\Api\Enterprise;

use App\Contracts\SystemResourceMonitorInterface;
use App\Http\Controllers\Controller;
use App\Models\Server;
use App\Models\Organization;
use Carbon\Carbon;
use Illuminate\Http\Request;
use Illuminate\Http\JsonResponse;

class MetricsController extends Controller
{
    public function __construct(
        private SystemResourceMonitorInterface $monitor
    ) {}

    /**
     * Get server metrics
     *
     * @param Request $request
     * @param Server $server
     * @return JsonResponse
     */
    public function serverMetrics(Request $request, Server $server): JsonResponse
    {
        $this->authorize('view', $server);

        $validated = $request->validate([
            'start_time' => 'required|date',
            'end_time' => 'required|date|after:start_time',
            'granularity' => 'required|in:1min,5min,1hour,1day',
        ]);

        $metrics = $this->monitor->getServerMetrics(
            $server,
            Carbon::parse($validated['start_time']),
            Carbon::parse($validated['end_time']),
            $validated['granularity']
        );

        return response()->json([
            'server_id' => $server->id,
            'server_name' => $server->name,
            'metrics' => $metrics,
        ]);
    }

    /**
     * Get organization metrics
     *
     * @param Request $request
     * @param Organization $organization
     * @return JsonResponse
     */
    public function organizationMetrics(Request $request, Organization $organization): JsonResponse
    {
        $this->authorize('view', $organization);

        $validated = $request->validate([
            'start_time' => 'required|date',
            'end_time' => 'required|date|after:start_time',
            'granularity' => 'required|in:5min,1hour,1day',
        ]);

        $metrics = $this->monitor->getOrganizationMetrics(
            $organization,
            Carbon::parse($validated['start_time']),
            Carbon::parse($validated['end_time']),
            $validated['granularity']
        );

        return response()->json([
            'organization_id' => $organization->id,
            'organization_name' => $organization->name,
            'metrics' => $metrics,
        ]);
    }

    /**
     * Detect anomalies
     *
     * @param Server $server
     * @return JsonResponse
     */
    public function anomalies(Server $server): JsonResponse
    {
        $this->authorize('view', $server);

        $anomalies = $this->monitor->detectAnomalies($server, Carbon::now());

        return response()->json([
            'server_id' => $server->id,
            'anomalies' => $anomalies,
            'has_anomalies' => count($anomalies) > 0,
        ]);
    }

    /**
     * Get capacity forecast
     *
     * @param Request $request
     * @param Server $server
     * @return JsonResponse
     */
    public function forecast(Request $request, Server $server): JsonResponse
    {
        $this->authorize('view', $server);

        $validated = $request->validate([
            'metric' => 'required|in:cpu,memory,disk',
            'forecast_days' => 'integer|min:1|max:30',
        ]);

        $forecast = $this->monitor->generateCapacityForecast(
            $server,
            $validated['metric'],
            $validated['forecast_days'] ?? 7
        );

        return response()->json([
            'server_id' => $server->id,
            'metric' => $validated['metric'],
            'forecast' => $forecast,
        ]);
    }
}
```

### Model Enhancement - MetricAggregation

**File:** `app/Models/Enterprise/MetricAggregation.php`

```php
<?php

namespace App\Models\Enterprise;

use App\Models\Server;
use App\Models\Organization;
use Illuminate\Database\Eloquent\Model;
use Illuminate\Database\Eloquent\Relations\BelongsTo;

class MetricAggregation extends Model
{
    protected $fillable = [
        'server_id',
        'organization_id',
        'bucket_start',
        'bucket_end',
        'granularity',
        'cpu_avg',
        'cpu_min',
        'cpu_max',
        'cpu_p50',
        'cpu_p95',
        'cpu_p99',
        'memory_avg',
        'memory_min',
        'memory_max',
        'memory_p50',
        'memory_p95',
        'memory_p99',
        'disk_avg',
        'disk_min',
        'disk_max',
        'disk_p50',
        'disk_p95',
        'disk_p99',
        'network_in_avg',
        'network_in_max',
        'network_out_avg',
        'network_out_max',
        'sample_count',
    ];

    protected $casts = [
        'bucket_start' => 'datetime',
        'bucket_end' => 'datetime',
        'cpu_avg' => 'decimal:2',
        'cpu_min' => 'decimal:2',
        'cpu_max' => 'decimal:2',
        'cpu_p50' => 'decimal:2',
        'cpu_p95' => 'decimal:2',
        'cpu_p99' => 'decimal:2',
    ];

    public function server(): BelongsTo
    {
        return $this->belongsTo(Server::class);
    }

    public function organization(): BelongsTo
    {
        return $this->belongsTo(Organization::class);
    }

    /**
     * Scope for recent metrics
     */
    public function scopeRecent($query, int $hours = 24)
    {
        return $query->where('bucket_start', '>=', now()->subHours($hours));
    }

    /**
     * Scope for specific granularity
     */
    public function scopeGranularity($query, string $granularity)
    {
        return $query->where('granularity', $granularity);
    }
}
```

### Schedule Configuration

Add to `app/Console/Kernel.php`:

```php
protected function schedule(Schedule $schedule): void
{
    // Aggregate 1-minute metrics every 5 minutes
    $schedule->job(new AggregateMetricsJob('1min'))
        ->everyFiveMinutes()
        ->withoutOverlapping();

    // Aggregate 5-minute metrics every hour
    $schedule->job(new AggregateMetricsJob('5min'))
        ->hourly()
        ->withoutOverlapping();

    // Aggregate hourly metrics daily
    $schedule->job(new AggregateMetricsJob('1hour'))
        ->dailyAt('02:00')
        ->withoutOverlapping();

    // Aggregate daily metrics weekly
    $schedule->job(new AggregateMetricsJob('1day'))
        ->weekly()
        ->sundays()
        ->at('03:00')
        ->withoutOverlapping();

    // Clean up old metrics daily
    $schedule->job(new CleanupOldMetricsJob())
        ->dailyAt('04:00')
        ->withoutOverlapping();
}
```

## Implementation Approach

### Step 1: Create Database Schema
1. Create migration for `metric_aggregations` table
2. Add indexes for time-series queries
3. Consider PostgreSQL partitioning for large-scale deployments
4. Run migration: `php artisan migrate`

### Step 2: Create Service Interface and Implementation
1. Create `SystemResourceMonitorInterface` in `app/Contracts/`
2. Implement `SystemResourceMonitor` service in `app/Services/Enterprise/`
3. Register service in `EnterpriseServiceProvider`
4. Add configuration for cache TTL and retention policies

### Step 3: Implement Core Aggregation Logic
1. Implement `aggregateMetrics()` with percentile calculations
2. Add database-specific percentile queries (PostgreSQL, MySQL support)
3. Handle edge cases (no data, single data point, etc.)
4. Add comprehensive error logging

### Step 4: Implement Query Methods
1. Create `getServerMetrics()` with Redis caching
2. Create `getOrganizationMetrics()` with hierarchical aggregation
3. Add pagination support for large datasets
4. Implement cache invalidation on new aggregations

### Step 5: Add Analytical Features
1. Implement `detectAnomalies()` with threshold detection
2. Implement `generateCapacityForecast()` with linear regression
3. Add confidence scoring for forecasts
4. Test with historical data

### Step 6: Create Background Jobs
1. Create `AggregateMetricsJob` with granularity support
2. Create `CleanupOldMetricsJob` with retention policies
3. Add to Laravel scheduler with appropriate frequencies
4. Configure job queues (use 'monitoring' queue)

### Step 7: Build API Endpoints
1. Create `MetricsController` with authentication
2. Add endpoints for server and organization metrics
3. Add anomaly detection and forecasting endpoints
4. Implement rate limiting for API calls

### Step 8: Testing and Optimization
1. Write unit tests for all service methods
2. Write integration tests for API endpoints
3. Performance testing with large datasets
4. Optimize queries with EXPLAIN ANALYZE
5. Benchmark Redis cache hit rates

## Test Strategy

### Unit Tests

**File:** `tests/Unit/Enterprise/SystemResourceMonitorTest.php`

```php
<?php

use App\Services\Enterprise\SystemResourceMonitor;
use App\Models\Server;
use App\Models\Enterprise\ServerResourceMetric;
use App\Models\Enterprise\MetricAggregation;
use Carbon\Carbon;
use Illuminate\Support\Facades\Cache;

beforeEach(function () {
    $this->monitor = app(SystemResourceMonitor::class);
});

it('aggregates metrics correctly for 5-minute buckets', function () {
    $server = Server::factory()->create();
    $startTime = Carbon::now()->subHour();

    // Create test metrics
    for ($i = 0; $i < 10; $i++) {
        ServerResourceMetric::factory()->create([
            'server_id' => $server->id,
            'collected_at' => $startTime->copy()->addMinutes($i),
            'cpu_usage_percent' => 50 + $i,
            'memory_used_bytes' => 1000000000 + ($i * 10000000),
        ]);
    }

    $aggregated = $this->monitor->aggregateMetrics(
        '5min',
        $startTime,
        $startTime->copy()->addMinutes(10)
    );

    expect($aggregated)->toBeGreaterThan(0);

    $aggregation = MetricAggregation::where('server_id', $server->id)
        ->where('granularity', '5min')
        ->first();

    expect($aggregation)->not->toBeNull();
    expect($aggregation->cpu_avg)->toBeGreaterThan(50);
    expect($aggregation->sample_count)->toBe(5);
});

it('detects anomalies correctly', function () {
    $server = Server::factory()->create();
    $baseTime = Carbon::now();

    // Create baseline metrics (normal usage)
    for ($i = 0; $i < 12; $i++) {
        ServerResourceMetric::factory()->create([
            'server_id' => $server->id,
            'collected_at' => $baseTime->copy()->subHour()->addMinutes($i * 5),
            'cpu_usage_percent' => 30,
        ]);
    }

    // Create aggregations
    $this->monitor->aggregateMetrics(
        '5min',
        $baseTime->copy()->subHour(),
        $baseTime
    );

    // Create anomaly (spike to 90%)
    ServerResourceMetric::factory()->create([
        'server_id' => $server->id,
        'collected_at' => $baseTime,
        'cpu_usage_percent' => 90,
    ]);

    $anomalies = $this->monitor->detectAnomalies($server, $baseTime);

    expect($anomalies)->toHaveCount(1);
    expect($anomalies[0]['metric'])->toBe('cpu');
    expect($anomalies[0]['change_percent'])->toBeGreaterThan(50);
});

it('generates capacity forecast', function () {
    $server = Server::factory()->create();
    $startTime = Carbon::now()->subDays(30);

    // Create trending data (increasing CPU usage)
    for ($i = 0; $i < 30; $i++) {
        MetricAggregation::factory()->create([
            'server_id' => $server->id,
            'granularity' => '1day',
            'bucket_start' => $startTime->copy()->addDays($i),
            'cpu_avg' => 20 + ($i * 1.5), // Gradually increasing
        ]);
    }

    $forecast = $this->monitor->generateCapacityForecast($server, 'cpu', 7);

    expect($forecast)->toHaveKeys(['forecast', 'trend', 'confidence']);
    expect($forecast['trend'])->toBe('increasing');
    expect($forecast['forecast'])->toHaveCount(7);
    expect($forecast['confidence'])->toBeGreaterThan(0.5);
});

it('caches server metrics queries', function () {
    $server = Server::factory()->create();
    $startTime = Carbon::now()->subHour();
    $endTime = Carbon::now();

    MetricAggregation::factory()->create([
        'server_id' => $server->id,
        'granularity' => '5min',
        'bucket_start' => $startTime,
    ]);

    // First call should hit database
    $metrics1 = $this->monitor->getServerMetrics($server, $startTime, $endTime, '5min');

    // Second call should hit cache
    Cache::shouldReceive('remember')->once();
    $metrics2 = $this->monitor->getServerMetrics($server, $startTime, $endTime, '5min');

    expect($metrics1->count())->toBe($metrics2->count());
});

it('cleans up old metrics according to retention policy', function () {
    $server = Server::factory()->create();

    // Create old raw metrics (8 days ago - should be deleted)
    ServerResourceMetric::factory()->create([
        'server_id' => $server->id,
        'collected_at' => Carbon::now()->subDays(8),
    ]);

    // Create recent metrics (5 days ago - should be kept)
    ServerResourceMetric::factory()->create([
        'server_id' => $server->id,
        'collected_at' => Carbon::now()->subDays(5),
    ]);

    $stats = $this->monitor->cleanupOldMetrics();

    expect($stats['raw_deleted'])->toBeGreaterThan(0);
    expect(ServerResourceMetric::count())->toBe(1);
});
```

### Integration Tests

**File:** `tests/Feature/Enterprise/MetricsApiTest.php`

```php
<?php

use App\Models\Server;
use App\Models\User;
use App\Models\Organization;
use App\Models\Enterprise\MetricAggregation;
use Carbon\Carbon;

it('retrieves server metrics via API', function () {
    $organization = Organization::factory()->create();
    $user = User::factory()->create();
    $organization->users()->attach($user, ['role' => 'admin']);

    $server = Server::factory()->create(['team_id' => $organization->id]);

    $startTime = Carbon::now()->subHour();

    MetricAggregation::factory()->count(12)->create([
        'server_id' => $server->id,
        'organization_id' => $organization->id,
        'granularity' => '5min',
        'bucket_start' => fn() => $startTime->copy()->addMinutes(fake()->numberBetween(0, 55)),
    ]);

    $response = $this->actingAs($user)->getJson("/api/enterprise/metrics/servers/{$server->id}", [
        'start_time' => $startTime->toIso8601String(),
        'end_time' => Carbon::now()->toIso8601String(),
        'granularity' => '5min',
    ]);

    $response->assertOk()
        ->assertJsonStructure([
            'server_id',
            'server_name',
            'metrics' => [
                '*' => ['timestamp', 'cpu', 'memory', 'disk', 'network']
            ]
        ]);
});

it('detects anomalies via API', function () {
    $organization = Organization::factory()->create();
    $user = User::factory()->create();
    $organization->users()->attach($user, ['role' => 'admin']);

    $server = Server::factory()->create(['team_id' => $organization->id]);

    $response = $this->actingAs($user)->getJson("/api/enterprise/metrics/servers/{$server->id}/anomalies");

    $response->assertOk()
        ->assertJsonStructure([
            'server_id',
            'anomalies',
            'has_anomalies'
        ]);
});

it('generates capacity forecast via API', function () {
    $organization = Organization::factory()->create();
    $user = User::factory()->create();
    $organization->users()->attach($user, ['role' => 'admin']);

    $server = Server::factory()->create(['team_id' => $organization->id]);

    $response = $this->actingAs($user)->getJson("/api/enterprise/metrics/servers/{$server->id}/forecast", [
        'metric' => 'cpu',
        'forecast_days' => 7,
    ]);

    $response->assertOk()
        ->assertJsonStructure([
            'server_id',
            'metric',
            'forecast' => [
                'forecast',
                'trend',
                'confidence'
            ]
        ]);
});

it('enforces authorization for server metrics', function () {
    $organization = Organization::factory()->create();
    $otherOrg = Organization::factory()->create();
    $user = User::factory()->create();
    $otherOrg->users()->attach($user, ['role' => 'admin']);

    $server = Server::factory()->create(['team_id' => $organization->id]);

    $response = $this->actingAs($user)->getJson("/api/enterprise/metrics/servers/{$server->id}", [
        'start_time' => Carbon::now()->subHour()->toIso8601String(),
        'end_time' => Carbon::now()->toIso8601String(),
        'granularity' => '5min',
    ]);

    $response->assertForbidden();
});
```

### Performance Tests

**File:** `tests/Performance/MetricsAggregationPerformanceTest.php`

```php
<?php

use App\Services\Enterprise\SystemResourceMonitor;
use App\Models\Server;
use App\Models\Enterprise\ServerResourceMetric;
use Carbon\Carbon;

it('aggregates 10000 metrics in under 5 seconds', function () {
    $server = Server::factory()->create();
    $startTime = Carbon::now()->subHour();

    // Create 10,000 metric samples
    $metrics = [];
    for ($i = 0; $i < 10000; $i++) {
        $metrics[] = [
            'server_id' => $server->id,
            'collected_at' => $startTime->copy()->addSeconds($i * 3.6), // Every 3.6 seconds
            'cpu_usage_percent' => fake()->numberBetween(20, 80),
            'memory_used_bytes' => fake()->numberBetween(1000000000, 8000000000),
            'disk_used_bytes' => fake()->numberBetween(10000000000, 100000000000),
        ];
    }

    ServerResourceMetric::insert($metrics);

    $monitor = app(SystemResourceMonitor::class);

    $startBenchmark = microtime(true);

    $aggregated = $monitor->aggregateMetrics(
        '5min',
        $startTime,
        $startTime->copy()->addHour()
    );

    $duration = microtime(true) - $startBenchmark;

    expect($duration)->toBeLessThan(5.0);
    expect($aggregated)->toBeGreaterThan(0);
});

it('retrieves dashboard data in under 200ms', function () {
    $server = Server::factory()->create();
    $startTime = Carbon::now()->subDay();

    // Create aggregated data for 24 hours (288 5-minute buckets)
    MetricAggregation::factory()->count(288)->create([
        'server_id' => $server->id,
        'granularity' => '5min',
        'bucket_start' => fn() => $startTime->copy()->addMinutes(fake()->numberBetween(0, 1435)),
    ]);

    $monitor = app(SystemResourceMonitor::class);

    $startBenchmark = microtime(true);

    $metrics = $monitor->getServerMetrics(
        $server,
        $startTime,
        Carbon::now(),
        '5min'
    );

    $duration = microtime(true) - $startBenchmark;

    expect($duration)->toBeLessThan(0.2); // 200ms
    expect($metrics->count())->toBeGreaterThan(0);
});
```

## Definition of Done

- [ ] SystemResourceMonitorInterface created with all required methods
- [ ] SystemResourceMonitor service implemented with full functionality
- [ ] Service registered in EnterpriseServiceProvider
- [ ] Database migration for metric_aggregations table created and run
- [ ] MetricAggregation model created with relationships
- [ ] Time-series aggregation logic implemented (1min, 5min, 1hour, 1day)
- [ ] Percentile calculations working for PostgreSQL and MySQL
- [ ] Redis caching layer implemented for recent metrics
- [ ] Data retention policies implemented with cleanup job
- [ ] Anomaly detection algorithm implemented and tested
- [ ] Capacity forecasting with linear regression implemented
- [ ] Organization-level metric aggregation working
- [ ] AggregateMetricsJob created and scheduled
- [ ] CleanupOldMetricsJob created and scheduled
- [ ] MetricsController API endpoints implemented
- [ ] API authentication and authorization working
- [ ] Database indexes created for optimal query performance
- [ ] Unit tests written (15+ tests, >90% coverage)
- [ ] Integration tests written (8+ tests)
- [ ] Performance tests passing (< 200ms dashboard queries)
- [ ] Load testing with 10,000+ metrics completed
- [ ] Code follows Laravel 12 and Coolify standards
- [ ] Laravel Pint formatting applied (`./vendor/bin/pint`)
- [ ] PHPStan level 5 passing with zero errors
- [ ] Documentation updated (PHPDoc blocks, service usage examples)
- [ ] Manual testing with real server data completed
- [ ] Code reviewed and approved

## Related Tasks

- **Depends on:** Task 23 (Enhanced ResourcesCheck for raw metric collection)
- **Enables:** Task 26 (CapacityManager uses aggregated metrics for scoring)
- **Enables:** Task 28 (Organization quota enforcement uses aggregated usage)
- **Enables:** Task 29 (ResourceDashboard.vue displays aggregated metrics)
- **Enables:** Task 30 (CapacityPlanner.vue uses forecasts for capacity planning)
- **Integrates with:** Task 24 (ResourceMonitoringJob triggers aggregation)
- **Integrates with:** Task 31 (WebSocket broadcasting for real-time updates)
